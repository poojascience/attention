{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97773f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T14:36:53.123532Z",
     "iopub.status.busy": "2024-08-20T14:36:53.122770Z",
     "iopub.status.idle": "2024-08-20T14:37:34.223628Z",
     "shell.execute_reply": "2024-08-20T14:37:34.222449Z"
    },
    "papermill": {
     "duration": 41.109425,
     "end_time": "2024-08-20T14:37:34.226012",
     "exception": false,
     "start_time": "2024-08-20T14:36:53.116587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting indic-nlp-library\n",
      "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting sphinx-argparse (from indic-nlp-library)\n",
      "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
      "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting morfessor (from indic-nlp-library)\n",
      "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
      "Requirement already satisfied: pandas in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from indic-nlp-library) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from indic-nlp-library) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->indic-nlp-library) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->indic-nlp-library) (2024.1)\n",
      "Collecting sphinx>=5.1.0 (from sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinx-8.0.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting docutils>=0.19 (from sphinx-argparse->indic-nlp-library)\n",
      "  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sphinx>=5.1.0 (from sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinx-7.4.7-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting docutils>=0.19 (from sphinx-argparse->indic-nlp-library)\n",
      "  Downloading docutils-0.20.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n",
      "Collecting sphinxcontrib-applehelp (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-devhelp (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-jsmath (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting sphinxcontrib-qthelp (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: Jinja2>=3.1 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.4)\n",
      "Requirement already satisfied: Pygments>=2.17 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
      "Collecting snowballstemmer>=2.2 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting babel>=2.13 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading babel-2.16.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting alabaster~=0.7.14 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting imagesize>=1.3 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: requests>=2.30.0 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
      "Requirement already satisfied: packaging>=23.0 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.1)\n",
      "Requirement already satisfied: colorama>=0.4.6 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2024.7.4)\n",
      "Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
      "   ---------------------------------------- 0.0/40.3 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 10.2/40.3 kB ? eta -:--:--\n",
      "   ------------------------------ --------- 30.7/40.3 kB 435.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 40.3/40.3 kB 384.9 kB/s eta 0:00:00\n",
      "Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
      "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
      "Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.8 MB 991.0 kB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.1/2.8 MB 1.3 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.2/2.8 MB 1.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.8 MB 2.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.7/2.8 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.1/2.8 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.1/2.8 MB 6.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.7/2.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 7.5 MB/s eta 0:00:00\n",
      "Downloading docutils-0.20.1-py3-none-any.whl (572 kB)\n",
      "   ---------------------------------------- 0.0/572.7 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 471.0/572.7 kB ? eta -:--:--\n",
      "   ---------------------------------------  563.2/572.7 kB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 572.7/572.7 kB 5.1 MB/s eta 0:00:00\n",
      "Downloading sphinx-7.4.7-py3-none-any.whl (3.4 MB)\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 1.0/3.4 MB 32.7 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.0/3.4 MB 32.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.6/3.4 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.1/3.4 MB 22.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.4/3.4 MB 15.5 MB/s eta 0:00:00\n",
      "Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "   ---------------------------------------- 0.0/121.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 121.1/121.1 kB 6.9 MB/s eta 0:00:00\n",
      "Downloading alabaster-0.7.16-py3-none-any.whl (13 kB)\n",
      "Downloading babel-2.16.0-py3-none-any.whl (9.6 MB)\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.7/9.6 MB 53.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.7/9.6 MB 53.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.7/9.6 MB 53.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.7/9.6 MB 53.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.7/9.6 MB 53.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.6/9.6 MB 24.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.3/9.6 MB 22.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.9/9.6 MB 24.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.9/9.6 MB 24.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.1/9.6 MB 20.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.6/9.6 MB 22.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.6/9.6 MB 20.4 MB/s eta 0:00:00\n",
      "Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "   ---------------------------------------- 0.0/93.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 93.0/93.0 kB ? eta 0:00:00\n",
      "Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n",
      "   ---------------------------------------- 0.0/98.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 98.7/98.7 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n",
      "   ---------------------------------------- 0.0/92.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 92.1/92.1 kB 5.1 MB/s eta 0:00:00\n",
      "Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n",
      "   ---------------------------------------- 0.0/119.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 119.3/119.3 kB 6.8 MB/s eta 0:00:00\n",
      "Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n",
      "   ---------------------------------------- 0.0/82.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 82.5/82.5 kB ? eta 0:00:00\n",
      "Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n",
      "   ---------------------------------------- 0.0/88.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 88.7/88.7 kB 4.9 MB/s eta 0:00:00\n",
      "Installing collected packages: snowballstemmer, morfessor, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, imagesize, docutils, babel, alabaster, sphinx, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
      "Successfully installed alabaster-0.7.16 babel-2.16.0 docutils-0.20.1 imagesize-1.4.1 indic-nlp-library-0.92 morfessor-2.0.6 snowballstemmer-2.2.0 sphinx-7.4.7 sphinx-argparse-0.5.2 sphinx-rtd-theme-2.0.0 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\pooja\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.4.2)\n",
      "Requirement already satisfied: portalocker in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sacrebleu) (2.10.1)\n",
      "Requirement already satisfied: regex in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sacrebleu) (2024.5.15)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sacrebleu) (5.2.2)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from portalocker->sacrebleu) (306)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\pooja\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\pooja\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.5 MB 435.7 kB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.1/1.5 MB 558.5 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.5/1.5 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.9/1.5 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 4.5 MB/s eta 0:00:00\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\pooja\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install indic-nlp-library\n",
    "!pip install sacrebleu\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b07e861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\pooja\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ac8b93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T14:37:34.242938Z",
     "iopub.status.busy": "2024-08-20T14:37:34.242654Z",
     "iopub.status.idle": "2024-08-20T14:37:40.326607Z",
     "shell.execute_reply": "2024-08-20T14:37:40.325626Z"
    },
    "papermill": {
     "duration": 6.094894,
     "end_time": "2024-08-20T14:37:40.328939",
     "exception": false,
     "start_time": "2024-08-20T14:37:34.234045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# import sentencepiece as spm\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mindicnlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m indic_tokenize  \n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import sentencepiece as spm\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from indicnlp.tokenize import indic_tokenize  \n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import tqdm as notebook_tqdm\n",
    "import json\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c7923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T14:37:40.345551Z",
     "iopub.status.busy": "2024-08-20T14:37:40.345133Z",
     "iopub.status.idle": "2024-08-20T14:37:40.501188Z",
     "shell.execute_reply": "2024-08-20T14:37:40.500340Z"
    },
    "papermill": {
     "duration": 0.16701,
     "end_time": "2024-08-20T14:37:40.503801",
     "exception": false,
     "start_time": "2024-08-20T14:37:40.336791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14076\n",
      "14076\n",
      "11044 582\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hindi</th>\n",
       "      <th>Bhili</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6618</th>\n",
       "      <td>मुझे आलू उबालकर खाना पसंद है ।</td>\n",
       "      <td>मन बटांका उका‍नीन खावानू असेल लाग छे ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11729</th>\n",
       "      <td>और आज देश में व्‍यापक रूप से चर्चा चल रही है, ...</td>\n",
       "      <td>एक देह’ मा एक हाते सुनाव” ये वात पण थावी जोवे।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3420</th>\n",
       "      <td>मुझे अजीब लग रहा है।</td>\n",
       "      <td>मेसे वारू निही लागे ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6445</th>\n",
       "      <td>तुम जाकर पौधों को छुओ।</td>\n",
       "      <td>तमे जाय ने तरू ने धरो ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5750</th>\n",
       "      <td>तुम्‍हारी बहन को क्‍या पसंद है।</td>\n",
       "      <td>तारो बोहन काय काय पसंद दे</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>मैं अभी आई।</td>\n",
       "      <td>हूँ हमणेस आवी।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8647</th>\n",
       "      <td>वह दूध पीता है।</td>\n",
       "      <td>पेलो दुध पितेलो</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429</th>\n",
       "      <td>क्‍या सब ठीक है।</td>\n",
       "      <td>कायरे सुब वारलूत की ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>फलदार पेड़ों के नाम की सूची बनाइए।</td>\n",
       "      <td>फलदार झाडवा ना नाम नी सूची बणावो</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13681</th>\n",
       "      <td>मैं आपके बीच प्रधान मंत्री के रूप में नहीं, प्...</td>\n",
       "      <td>हुं तमारे वस मा प्रधान मंत्री ना वेह मा कोईन, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12201</th>\n",
       "      <td>लेकिन सबसे ज्यादा एक बात मुझे कहनी है ये स्वच्...</td>\n",
       "      <td>पण हंगला थी वदु एक वात मने केहवी से यो साफ सफा...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10818</th>\n",
       "      <td>जब सरकार, किसी काम में विलंब हो जाता है, तो सि...</td>\n",
       "      <td>जैर सरकार ने करना काम मा बार थाई जाई से; तैर ख...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>बहुत खुशबूदार फूल है इस बगिया में।</td>\n",
       "      <td>असेल खुशबुदार फुलू स इना बगीचा मा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11009</th>\n",
       "      <td>आखिकार 1947 में देश आजाद हुआ, हजार साल की गुला...</td>\n",
       "      <td>आखिर कार 1947 मां देश आजाद थायो, हजार वरेह नीं...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12616</th>\n",
       "      <td>हर एक वैज्ञानिक जो हमारे देश के लिए इनोवेशन क...</td>\n",
       "      <td>हंगला जाणकार आपडा देह नो क्रम विकास करे से।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>रानी ने डटकर उसका सामना किया।</td>\n",
       "      <td>रानी भी हिनाने सामे जाय</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12045</th>\n",
       "      <td>हमारे वैज्ञानिकों की सिद्धि है।\\n</td>\n",
       "      <td>आपड़ा मोटा घणा मोटा (वैज्ञानिक) नी यी कळियों से।\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5326</th>\n",
       "      <td>पानी ठंडा है। ठंडे पानी से नहीं नहा पाऊगां।</td>\n",
       "      <td>पाणी टाडू छे टाडे पाणी ही नी उगलवानू ।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10327</th>\n",
       "      <td>लोगों को, जन-औषधि केन्द्रों में, सस्ती दरों पर...</td>\n",
       "      <td>लोकुने दवाखाने कमरूपया थी दवादारू मकी री से।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4936</th>\n",
       "      <td>नाल काटने के बाद बच्‍चे को गुनगुने पानी से नहल...</td>\n",
       "      <td>डुटी काटणें बाद पुरीया काजे फोलकें पाणीयें उगं...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Hindi  \\\n",
       "6618                      मुझे आलू उबालकर खाना पसंद है ।   \n",
       "11729  और आज देश में व्‍यापक रूप से चर्चा चल रही है, ...   \n",
       "3420                                मुझे अजीब लग रहा है।   \n",
       "6445                              तुम जाकर पौधों को छुओ।   \n",
       "5750                     तुम्‍हारी बहन को क्‍या पसंद है।   \n",
       "33                                           मैं अभी आई।   \n",
       "8647                                     वह दूध पीता है।   \n",
       "3429                                    क्‍या सब ठीक है।   \n",
       "665                    फलदार पेड़ों के नाम की सूची बनाइए।   \n",
       "13681  मैं आपके बीच प्रधान मंत्री के रूप में नहीं, प्...   \n",
       "12201  लेकिन सबसे ज्यादा एक बात मुझे कहनी है ये स्वच्...   \n",
       "10818  जब सरकार, किसी काम में विलंब हो जाता है, तो सि...   \n",
       "1159                  बहुत खुशबूदार फूल है इस बगिया में।   \n",
       "11009  आखिकार 1947 में देश आजाद हुआ, हजार साल की गुला...   \n",
       "12616   हर एक वैज्ञानिक जो हमारे देश के लिए इनोवेशन क...   \n",
       "1734                       रानी ने डटकर उसका सामना किया।   \n",
       "12045                  हमारे वैज्ञानिकों की सिद्धि है।\\n   \n",
       "5326         पानी ठंडा है। ठंडे पानी से नहीं नहा पाऊगां।   \n",
       "10327  लोगों को, जन-औषधि केन्द्रों में, सस्ती दरों पर...   \n",
       "4936   नाल काटने के बाद बच्‍चे को गुनगुने पानी से नहल...   \n",
       "\n",
       "                                                   Bhili  \n",
       "6618             मन बटांका उका‍नीन खावानू असेल लाग छे ।   \n",
       "11729     एक देह’ मा एक हाते सुनाव” ये वात पण थावी जोवे।  \n",
       "3420                               मेसे वारू निही लागे ।  \n",
       "6445                            तमे जाय ने तरू ने धरो ।   \n",
       "5750                           तारो बोहन काय काय पसंद दे  \n",
       "33                                        हूँ हमणेस आवी।  \n",
       "8647                                     पेलो दुध पितेलो  \n",
       "3429                               कायरे सुब वारलूत की ।  \n",
       "665                     फलदार झाडवा ना नाम नी सूची बणावो  \n",
       "13681  हुं तमारे वस मा प्रधान मंत्री ना वेह मा कोईन, ...  \n",
       "12201  पण हंगला थी वदु एक वात मने केहवी से यो साफ सफा...  \n",
       "10818  जैर सरकार ने करना काम मा बार थाई जाई से; तैर ख...  \n",
       "1159                   असेल खुशबुदार फुलू स इना बगीचा मा  \n",
       "11009  आखिर कार 1947 मां देश आजाद थायो, हजार वरेह नीं...  \n",
       "12616       हंगला जाणकार आपडा देह नो क्रम विकास करे से।   \n",
       "1734                             रानी भी हिनाने सामे जाय  \n",
       "12045  आपड़ा मोटा घणा मोटा (वैज्ञानिक) नी यी कळियों से।\\n  \n",
       "5326              पाणी टाडू छे टाडे पाणी ही नी उगलवानू ।  \n",
       "10327       लोकुने दवाखाने कमरूपया थी दवादारू मकी री से।  \n",
       "4936   डुटी काटणें बाद पुरीया काजे फोलकें पाणीयें उगं...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df= pd.read_csv('/kaggle/input/comb-modi/Combined_data.csv')\n",
    "# df = df.drop('Unnamed: 2', axis=1)\n",
    "print(len(df))\n",
    "df.dropna()\n",
    "print(len(df))\n",
    "\n",
    "# Identify duplicates in each column\n",
    "duplicates_col1 = df.duplicated('Hindi', keep=False)\n",
    "duplicates_col2 = df.duplicated('Bhili', keep=False)\n",
    "\n",
    "# Combine the conditions to identify rows where either column has duplicates\n",
    "duplicates_any_col = duplicates_col1 | duplicates_col2\n",
    "df_cleaned = df[~duplicates_any_col]\n",
    "\n",
    "len(df_cleaned)\n",
    "df= df_cleaned\n",
    "len(df)\n",
    "df_train, df_val = train_test_split(df, test_size=0.05, random_state=42, shuffle= True)\n",
    "print(len(df_train), len(df_val))\n",
    "df_train.head(n=20)\n",
    "df_train.tail(n=20)\n",
    "df_val.tail(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f69fdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T14:37:40.521690Z",
     "iopub.status.busy": "2024-08-20T14:37:40.521391Z",
     "iopub.status.idle": "2024-08-20T22:28:42.900873Z",
     "shell.execute_reply": "2024-08-20T22:28:42.899996Z"
    },
    "papermill": {
     "duration": 28262.391745,
     "end_time": "2024-08-20T22:28:42.903744",
     "exception": false,
     "start_time": "2024-08-20T14:37:40.511999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11044 582\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11044 entries, 871 to 8658\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Hindi   11044 non-null  object\n",
      " 1   Bhili   11044 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 258.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 582 entries, 13501 to 4936\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Hindi   582 non-null    object\n",
      " 1   Bhili   582 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 13.6+ KB\n",
      "None None\n",
      "11044\n",
      "582\n",
      "tensor([[   9,  474,  135,  ...,    2,    2,    2],\n",
      "        [5055,   31,  362,  ...,    2,    2,    2],\n",
      "        [ 268,   88,  398,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [8117, 8120, 8121,  ...,    2,    2,    2],\n",
      "        [6318,  114, 3752,  ...,    2,    2,    2],\n",
      "        [  47, 2668,  477,  ...,    2,    2,    2]])\n",
      "12730 23730\n",
      "enc dim : 16 | dec dim : 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train Loss: 9.091 | Val Loss : 7.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train Loss: 7.343 | Val Loss : 7.356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train Loss: 7.105 | Val Loss : 7.292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train Loss: 6.952 | Val Loss : 7.272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Train Loss: 6.836 | Val Loss : 7.283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Train Loss: 6.747 | Val Loss : 7.259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Train Loss: 6.642 | Val Loss : 7.267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Train Loss: 6.565 | Val Loss : 7.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Train Loss: 6.484 | Val Loss : 7.323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Train Loss: 6.401 | Val Loss : 7.342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Train Loss: 6.333 | Val Loss : 7.405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Train Loss: 6.276 | Val Loss : 7.403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Train Loss: 6.204 | Val Loss : 7.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Train Loss: 6.154 | Val Loss : 7.477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Train Loss: 6.084 | Val Loss : 7.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Train Loss: 6.030 | Val Loss : 7.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Train Loss: 5.975 | Val Loss : 7.584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Train Loss: 5.923 | Val Loss : 7.530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Train Loss: 5.873 | Val Loss : 7.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Train Loss: 5.825 | Val Loss : 7.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Train Loss: 5.781 | Val Loss : 7.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Train Loss: 5.746 | Val Loss : 7.540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Train Loss: 5.719 | Val Loss : 7.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Train Loss: 5.674 | Val Loss : 7.576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Train Loss: 5.633 | Val Loss : 7.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Train Loss: 5.600 | Val Loss : 7.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Train Loss: 5.570 | Val Loss : 7.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Train Loss: 5.542 | Val Loss : 7.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Train Loss: 5.518 | Val Loss : 7.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Train Loss: 5.474 | Val Loss : 7.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Train Loss: 5.456 | Val Loss : 7.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Train Loss: 5.429 | Val Loss : 7.655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Train Loss: 5.402 | Val Loss : 7.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Train Loss: 5.376 | Val Loss : 7.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Train Loss: 5.356 | Val Loss : 7.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Train Loss: 5.333 | Val Loss : 7.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | Train Loss: 5.310 | Val Loss : 7.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 | Train Loss: 5.296 | Val Loss : 7.556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Train Loss: 5.261 | Val Loss : 7.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Train Loss: 5.235 | Val Loss : 7.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Train Loss: 5.223 | Val Loss : 7.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 | Train Loss: 5.196 | Val Loss : 7.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | Train Loss: 5.178 | Val Loss : 7.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 | Train Loss: 5.157 | Val Loss : 7.511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 | Train Loss: 5.145 | Val Loss : 7.513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Train Loss: 5.128 | Val Loss : 7.576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 | Train Loss: 5.109 | Val Loss : 7.488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | Train Loss: 5.095 | Val Loss : 7.520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 | Train Loss: 5.066 | Val Loss : 7.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Train Loss: 5.059 | Val Loss : 7.465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Train Loss: 5.035 | Val Loss : 7.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 | Train Loss: 5.036 | Val Loss : 7.528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 | Train Loss: 5.020 | Val Loss : 7.513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 | Train Loss: 5.006 | Val Loss : 7.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 | Train Loss: 4.990 | Val Loss : 7.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | Train Loss: 4.963 | Val Loss : 7.565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57 | Train Loss: 4.957 | Val Loss : 7.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 | Train Loss: 4.942 | Val Loss : 7.563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59 | Train Loss: 4.936 | Val Loss : 7.552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 | Train Loss: 4.919 | Val Loss : 7.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 | Train Loss: 4.915 | Val Loss : 7.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 | Train Loss: 4.902 | Val Loss : 7.553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63 | Train Loss: 4.884 | Val Loss : 7.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64 | Train Loss: 4.879 | Val Loss : 7.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 | Train Loss: 4.863 | Val Loss : 7.477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 | Train Loss: 4.851 | Val Loss : 7.520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 | Train Loss: 4.834 | Val Loss : 7.486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68 | Train Loss: 4.830 | Val Loss : 7.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Train Loss: 4.812 | Val Loss : 7.447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70 | Train Loss: 4.811 | Val Loss : 7.473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71 | Train Loss: 4.803 | Val Loss : 7.518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 | Train Loss: 4.789 | Val Loss : 7.525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 | Train Loss: 4.792 | Val Loss : 7.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74 | Train Loss: 4.772 | Val Loss : 7.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 | Train Loss: 4.760 | Val Loss : 7.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 | Train Loss: 4.686 | Val Loss : 7.379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 | Train Loss: 4.632 | Val Loss : 7.388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78 | Train Loss: 4.602 | Val Loss : 7.402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79 | Train Loss: 4.596 | Val Loss : 7.391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Train Loss: 4.586 | Val Loss : 7.414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81 | Train Loss: 4.566 | Val Loss : 7.412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82 | Train Loss: 4.558 | Val Loss : 7.422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83 | Train Loss: 4.549 | Val Loss : 7.422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84 | Train Loss: 4.537 | Val Loss : 7.429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85 | Train Loss: 4.532 | Val Loss : 7.431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86 | Train Loss: 4.522 | Val Loss : 7.429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 | Train Loss: 4.526 | Val Loss : 7.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88 | Train Loss: 4.526 | Val Loss : 7.444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 | Train Loss: 4.509 | Val Loss : 7.456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 | Train Loss: 4.516 | Val Loss : 7.452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91 | Train Loss: 4.506 | Val Loss : 7.442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92 | Train Loss: 4.504 | Val Loss : 7.451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93 | Train Loss: 4.502 | Val Loss : 7.477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94 | Train Loss: 4.493 | Val Loss : 7.472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 | Train Loss: 4.492 | Val Loss : 7.459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96 | Train Loss: 4.483 | Val Loss : 7.474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97 | Train Loss: 4.481 | Val Loss : 7.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98 | Train Loss: 4.484 | Val Loss : 7.449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 | Train Loss: 4.482 | Val Loss : 7.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | Train Loss: 4.477 | Val Loss : 7.452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101 | Train Loss: 4.471 | Val Loss : 7.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102 | Train Loss: 4.470 | Val Loss : 7.452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 103 | Train Loss: 4.469 | Val Loss : 7.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 104 | Train Loss: 4.462 | Val Loss : 7.472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 105 | Train Loss: 4.463 | Val Loss : 7.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 106 | Train Loss: 4.460 | Val Loss : 7.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 107 | Train Loss: 4.466 | Val Loss : 7.455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 108 | Train Loss: 4.452 | Val Loss : 7.464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 109 | Train Loss: 4.457 | Val Loss : 7.470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 110 | Train Loss: 4.454 | Val Loss : 7.456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 111 | Train Loss: 4.447 | Val Loss : 7.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 112 | Train Loss: 4.451 | Val Loss : 7.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 113 | Train Loss: 4.449 | Val Loss : 7.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 114 | Train Loss: 4.439 | Val Loss : 7.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 115 | Train Loss: 4.446 | Val Loss : 7.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 116 | Train Loss: 4.439 | Val Loss : 7.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 117 | Train Loss: 4.441 | Val Loss : 7.490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 118 | Train Loss: 4.441 | Val Loss : 7.463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 119 | Train Loss: 4.427 | Val Loss : 7.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 120 | Train Loss: 4.430 | Val Loss : 7.480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 121 | Train Loss: 4.431 | Val Loss : 7.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 122 | Train Loss: 4.421 | Val Loss : 7.480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 123 | Train Loss: 4.425 | Val Loss : 7.474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124 | Train Loss: 4.426 | Val Loss : 7.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125 | Train Loss: 4.420 | Val Loss : 7.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 126 | Train Loss: 4.413 | Val Loss : 7.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 127 | Train Loss: 4.413 | Val Loss : 7.480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 128 | Train Loss: 4.412 | Val Loss : 7.483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129 | Train Loss: 4.410 | Val Loss : 7.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 130 | Train Loss: 4.412 | Val Loss : 7.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 131 | Train Loss: 4.406 | Val Loss : 7.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 132 | Train Loss: 4.405 | Val Loss : 7.485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 133 | Train Loss: 4.409 | Val Loss : 7.474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 134 | Train Loss: 4.406 | Val Loss : 7.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 135 | Train Loss: 4.400 | Val Loss : 7.490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136 | Train Loss: 4.396 | Val Loss : 7.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 137 | Train Loss: 4.396 | Val Loss : 7.496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 138 | Train Loss: 4.403 | Val Loss : 7.476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 139 | Train Loss: 4.402 | Val Loss : 7.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 140 | Train Loss: 4.396 | Val Loss : 7.498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 141 | Train Loss: 4.392 | Val Loss : 7.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 142 | Train Loss: 4.392 | Val Loss : 7.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 143 | Train Loss: 4.396 | Val Loss : 7.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 144 | Train Loss: 4.383 | Val Loss : 7.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 145 | Train Loss: 4.386 | Val Loss : 7.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 146 | Train Loss: 4.381 | Val Loss : 7.486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 147 | Train Loss: 4.381 | Val Loss : 7.487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 148 | Train Loss: 4.382 | Val Loss : 7.521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 149 | Train Loss: 4.378 | Val Loss : 7.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150 | Train Loss: 4.384 | Val Loss : 7.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 151 | Train Loss: 4.373 | Val Loss : 7.520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 152 | Train Loss: 4.385 | Val Loss : 7.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 153 | Train Loss: 4.379 | Val Loss : 7.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 154 | Train Loss: 4.378 | Val Loss : 7.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 155 | Train Loss: 4.367 | Val Loss : 7.502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 156 | Train Loss: 4.377 | Val Loss : 7.494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 157 | Train Loss: 4.370 | Val Loss : 7.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 158 | Train Loss: 4.366 | Val Loss : 7.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 159 | Train Loss: 4.363 | Val Loss : 7.499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 160 | Train Loss: 4.368 | Val Loss : 7.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 161 | Train Loss: 4.366 | Val Loss : 7.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 162 | Train Loss: 4.358 | Val Loss : 7.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 163 | Train Loss: 4.372 | Val Loss : 7.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 164 | Train Loss: 4.356 | Val Loss : 7.506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 165 | Train Loss: 4.353 | Val Loss : 7.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 166 | Train Loss: 4.361 | Val Loss : 7.510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 167 | Train Loss: 4.355 | Val Loss : 7.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 168 | Train Loss: 4.350 | Val Loss : 7.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 169 | Train Loss: 4.353 | Val Loss : 7.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 170 | Train Loss: 4.353 | Val Loss : 7.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 171 | Train Loss: 4.354 | Val Loss : 7.516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 172 | Train Loss: 4.351 | Val Loss : 7.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 173 | Train Loss: 4.344 | Val Loss : 7.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 174 | Train Loss: 4.342 | Val Loss : 7.526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 175 | Train Loss: 4.350 | Val Loss : 7.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 176 | Train Loss: 4.339 | Val Loss : 7.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 177 | Train Loss: 4.351 | Val Loss : 7.540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 178 | Train Loss: 4.344 | Val Loss : 7.527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 179 | Train Loss: 4.346 | Val Loss : 7.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 180 | Train Loss: 4.340 | Val Loss : 7.520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 181 | Train Loss: 4.339 | Val Loss : 7.528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 182 | Train Loss: 4.340 | Val Loss : 7.525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 183 | Train Loss: 4.336 | Val Loss : 7.540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 184 | Train Loss: 4.329 | Val Loss : 7.530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 185 | Train Loss: 4.332 | Val Loss : 7.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 186 | Train Loss: 4.343 | Val Loss : 7.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 187 | Train Loss: 4.325 | Val Loss : 7.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 188 | Train Loss: 4.327 | Val Loss : 7.548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 189 | Train Loss: 4.328 | Val Loss : 7.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190 | Train Loss: 4.323 | Val Loss : 7.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 191 | Train Loss: 4.324 | Val Loss : 7.527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 192 | Train Loss: 4.329 | Val Loss : 7.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 193 | Train Loss: 4.323 | Val Loss : 7.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 194 | Train Loss: 4.326 | Val Loss : 7.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 195 | Train Loss: 4.325 | Val Loss : 7.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 196 | Train Loss: 4.323 | Val Loss : 7.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 197 | Train Loss: 4.319 | Val Loss : 7.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 198 | Train Loss: 4.323 | Val Loss : 7.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 199 | Train Loss: 4.324 | Val Loss : 7.545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200 | Train Loss: 4.322 | Val Loss : 7.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 201 | Train Loss: 4.315 | Val Loss : 7.535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 202 | Train Loss: 4.315 | Val Loss : 7.548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 203 | Train Loss: 4.314 | Val Loss : 7.545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 204 | Train Loss: 4.318 | Val Loss : 7.530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 205 | Train Loss: 4.306 | Val Loss : 7.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 206 | Train Loss: 4.308 | Val Loss : 7.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 207 | Train Loss: 4.307 | Val Loss : 7.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 208 | Train Loss: 4.309 | Val Loss : 7.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 209 | Train Loss: 4.307 | Val Loss : 7.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 210 | Train Loss: 4.303 | Val Loss : 7.551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 211 | Train Loss: 4.307 | Val Loss : 7.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 212 | Train Loss: 4.309 | Val Loss : 7.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 213 | Train Loss: 4.306 | Val Loss : 7.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 214 | Train Loss: 4.302 | Val Loss : 7.568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 215 | Train Loss: 4.304 | Val Loss : 7.552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 216 | Train Loss: 4.306 | Val Loss : 7.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 217 | Train Loss: 4.300 | Val Loss : 7.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 218 | Train Loss: 4.299 | Val Loss : 7.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 219 | Train Loss: 4.300 | Val Loss : 7.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 220 | Train Loss: 4.296 | Val Loss : 7.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 221 | Train Loss: 4.294 | Val Loss : 7.550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 222 | Train Loss: 4.298 | Val Loss : 7.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 223 | Train Loss: 4.289 | Val Loss : 7.570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 224 | Train Loss: 4.296 | Val Loss : 7.565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 225 | Train Loss: 4.297 | Val Loss : 7.569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 226 | Train Loss: 4.289 | Val Loss : 7.573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 227 | Train Loss: 4.294 | Val Loss : 7.552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 228 | Train Loss: 4.295 | Val Loss : 7.563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 229 | Train Loss: 4.295 | Val Loss : 7.563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 230 | Train Loss: 4.286 | Val Loss : 7.563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 231 | Train Loss: 4.285 | Val Loss : 7.563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 232 | Train Loss: 4.290 | Val Loss : 7.563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 233 | Train Loss: 4.286 | Val Loss : 7.566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 234 | Train Loss: 4.286 | Val Loss : 7.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 235 | Train Loss: 4.289 | Val Loss : 7.570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 236 | Train Loss: 4.281 | Val Loss : 7.580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 237 | Train Loss: 4.288 | Val Loss : 7.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 238 | Train Loss: 4.285 | Val Loss : 7.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 239 | Train Loss: 4.278 | Val Loss : 7.584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 240 | Train Loss: 4.286 | Val Loss : 7.565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 241 | Train Loss: 4.281 | Val Loss : 7.580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 242 | Train Loss: 4.278 | Val Loss : 7.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 243 | Train Loss: 4.284 | Val Loss : 7.579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 244 | Train Loss: 4.273 | Val Loss : 7.593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 245 | Train Loss: 4.278 | Val Loss : 7.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 246 | Train Loss: 4.275 | Val Loss : 7.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 247 | Train Loss: 4.278 | Val Loss : 7.573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 248 | Train Loss: 4.272 | Val Loss : 7.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 249 | Train Loss: 4.270 | Val Loss : 7.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 250 | Train Loss: 4.272 | Val Loss : 7.581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:17<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 251 | Train Loss: 4.264 | Val Loss : 7.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 252 | Train Loss: 4.269 | Val Loss : 7.587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 253 | Train Loss: 4.269 | Val Loss : 7.570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 254 | Train Loss: 4.268 | Val Loss : 7.582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 255 | Train Loss: 4.269 | Val Loss : 7.598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 256 | Train Loss: 4.260 | Val Loss : 7.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 257 | Train Loss: 4.256 | Val Loss : 7.591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 258 | Train Loss: 4.266 | Val Loss : 7.589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 259 | Train Loss: 4.261 | Val Loss : 7.576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 260 | Train Loss: 4.260 | Val Loss : 7.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 261 | Train Loss: 4.260 | Val Loss : 7.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 262 | Train Loss: 4.264 | Val Loss : 7.574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 263 | Train Loss: 4.263 | Val Loss : 7.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 264 | Train Loss: 4.257 | Val Loss : 7.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 265 | Train Loss: 4.261 | Val Loss : 7.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 266 | Train Loss: 4.261 | Val Loss : 7.589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 267 | Train Loss: 4.255 | Val Loss : 7.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 268 | Train Loss: 4.251 | Val Loss : 7.601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 269 | Train Loss: 4.254 | Val Loss : 7.578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 270 | Train Loss: 4.256 | Val Loss : 7.566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 271 | Train Loss: 4.248 | Val Loss : 7.580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 272 | Train Loss: 4.261 | Val Loss : 7.576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 273 | Train Loss: 4.252 | Val Loss : 7.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 274 | Train Loss: 4.257 | Val Loss : 7.604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 275 | Train Loss: 4.243 | Val Loss : 7.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 276 | Train Loss: 4.250 | Val Loss : 7.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 277 | Train Loss: 4.251 | Val Loss : 7.592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 278 | Train Loss: 4.252 | Val Loss : 7.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 279 | Train Loss: 4.247 | Val Loss : 7.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 280 | Train Loss: 4.246 | Val Loss : 7.590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 281 | Train Loss: 4.248 | Val Loss : 7.604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 282 | Train Loss: 4.258 | Val Loss : 7.604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 283 | Train Loss: 4.249 | Val Loss : 7.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 284 | Train Loss: 4.252 | Val Loss : 7.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 285 | Train Loss: 4.247 | Val Loss : 7.619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 286 | Train Loss: 4.246 | Val Loss : 7.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 287 | Train Loss: 4.248 | Val Loss : 7.593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 288 | Train Loss: 4.250 | Val Loss : 7.604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 289 | Train Loss: 4.244 | Val Loss : 7.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 290 | Train Loss: 4.238 | Val Loss : 7.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 291 | Train Loss: 4.238 | Val Loss : 7.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 292 | Train Loss: 4.247 | Val Loss : 7.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 293 | Train Loss: 4.238 | Val Loss : 7.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 294 | Train Loss: 4.246 | Val Loss : 7.601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 295 | Train Loss: 4.239 | Val Loss : 7.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 296 | Train Loss: 4.234 | Val Loss : 7.591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 297 | Train Loss: 4.236 | Val Loss : 7.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 298 | Train Loss: 4.231 | Val Loss : 7.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299 | Train Loss: 4.240 | Val Loss : 7.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300 | Train Loss: 4.232 | Val Loss : 7.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 301 | Train Loss: 4.234 | Val Loss : 7.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 302 | Train Loss: 4.240 | Val Loss : 7.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 303 | Train Loss: 4.230 | Val Loss : 7.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 304 | Train Loss: 4.229 | Val Loss : 7.619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 305 | Train Loss: 4.232 | Val Loss : 7.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 306 | Train Loss: 4.221 | Val Loss : 7.605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 307 | Train Loss: 4.219 | Val Loss : 7.619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 308 | Train Loss: 4.231 | Val Loss : 7.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 309 | Train Loss: 4.232 | Val Loss : 7.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 310 | Train Loss: 4.225 | Val Loss : 7.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 311 | Train Loss: 4.226 | Val Loss : 7.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 312 | Train Loss: 4.232 | Val Loss : 7.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 313 | Train Loss: 4.224 | Val Loss : 7.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 314 | Train Loss: 4.215 | Val Loss : 7.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 315 | Train Loss: 4.223 | Val Loss : 7.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 316 | Train Loss: 4.223 | Val Loss : 7.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 317 | Train Loss: 4.215 | Val Loss : 7.621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 318 | Train Loss: 4.220 | Val Loss : 7.619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 319 | Train Loss: 4.220 | Val Loss : 7.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 320 | Train Loss: 4.210 | Val Loss : 7.617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 321 | Train Loss: 4.219 | Val Loss : 7.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 322 | Train Loss: 4.215 | Val Loss : 7.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 323 | Train Loss: 4.212 | Val Loss : 7.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 324 | Train Loss: 4.216 | Val Loss : 7.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 325 | Train Loss: 4.220 | Val Loss : 7.621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 326 | Train Loss: 4.214 | Val Loss : 7.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 327 | Train Loss: 4.218 | Val Loss : 7.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 328 | Train Loss: 4.214 | Val Loss : 7.617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 329 | Train Loss: 4.215 | Val Loss : 7.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 330 | Train Loss: 4.221 | Val Loss : 7.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 331 | Train Loss: 4.213 | Val Loss : 7.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 332 | Train Loss: 4.209 | Val Loss : 7.621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 333 | Train Loss: 4.216 | Val Loss : 7.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 334 | Train Loss: 4.211 | Val Loss : 7.621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 335 | Train Loss: 4.201 | Val Loss : 7.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 336 | Train Loss: 4.203 | Val Loss : 7.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 337 | Train Loss: 4.206 | Val Loss : 7.639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 338 | Train Loss: 4.204 | Val Loss : 7.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 339 | Train Loss: 4.214 | Val Loss : 7.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 340 | Train Loss: 4.206 | Val Loss : 7.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 341 | Train Loss: 4.202 | Val Loss : 7.621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 342 | Train Loss: 4.200 | Val Loss : 7.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 343 | Train Loss: 4.203 | Val Loss : 7.640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 344 | Train Loss: 4.202 | Val Loss : 7.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:19<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 345 | Train Loss: 4.206 | Val Loss : 7.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 346 | Train Loss: 4.198 | Val Loss : 7.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 347 | Train Loss: 4.200 | Val Loss : 7.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 348 | Train Loss: 4.198 | Val Loss : 7.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 349 | Train Loss: 4.197 | Val Loss : 7.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [01:18<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 350 | Train Loss: 4.202 | Val Loss : 7.643\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12730 23730\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hindi_vocab={}\n",
    "bhili_vocab={}\n",
    "\n",
    "# hindi_set= set()\n",
    "\n",
    "hindi_vocab={'<start>':0, '<end>':1, '<pad>':2, '<unk>':3}\n",
    "bhili_vocab={'<start>':0, '<end>':1, '<pad>':2, '<unk>':3}\n",
    "ctr_h=4\n",
    "def build_vocab_hindi(indic_string, vocab):\n",
    "    global ctr_h\n",
    "#     words= text.split(\" \")\n",
    "    for word in indic_tokenize.trivial_tokenize(indic_string): \n",
    "        if word not in vocab:\n",
    "            vocab[word]= ctr_h\n",
    "            ctr_h+=1\n",
    "            \n",
    "            \n",
    "ctr_b=4\n",
    "def build_vocab_bhili(indic_string, vocab):\n",
    "    global ctr_b\n",
    "#     words= text.split(\" \")\n",
    "    for word in indic_tokenize.trivial_tokenize(indic_string): \n",
    "        if word not in vocab:\n",
    "            vocab[word]= ctr_b\n",
    "            ctr_b+=1\n",
    "# print(hindi_vocab)\n",
    "for index, row in df.iterrows():\n",
    "    text= row['Hindi']\n",
    "    build_vocab_hindi(text, hindi_vocab)\n",
    "#     print(f\"{index}==>{text}\")\n",
    "\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text= row['Bhili']\n",
    "#     print(text)\n",
    "    build_vocab_bhili(text, bhili_vocab)\n",
    "rev_hindi_vocab= {value: key for key, value in hindi_vocab.items()}\n",
    "rev_bhili_vocab= {value: key for key, value in bhili_vocab.items()}\n",
    "# print(rev_hindi_vocab)\n",
    "# print(rev_bhili_vocab)\n",
    "# print(len(hindi_vocab), len(bhili_vocab))\n",
    "# import json\n",
    "\n",
    "with open('/kaggle/working/hindi_vocab_chunked-speeches.json', 'w') as json_file:\n",
    "    json.dump(hindi_vocab, json_file)\n",
    "    \n",
    "with open('/kaggle/working/bhili_vocab_chunked-speeches.json', 'w') as json_file:\n",
    "    json.dump(bhili_vocab, json_file)\n",
    "\n",
    "def hindi_tensor_to_hindi_words(hindi_tensor):\n",
    "    hindi_tensor= hindi_tensor[0]\n",
    "    hindi_numpy_tensor= hindi_tensor.cpu().numpy()\n",
    "    list= [rev_hindi_vocab[item] for item in hindi_numpy_tensor]\n",
    "    return list \n",
    "\n",
    "def bhili_tensor_to_bhili_words(bhili_tensor):\n",
    "    bhili_tensor= bhili_tensor[0]\n",
    "    bhili_numpy_tensor= bhili_tensor.cpu().numpy()\n",
    "    list= [rev_bhili_vocab[item] for item in bhili_numpy_tensor]\n",
    "    return list \n",
    "\n",
    "def tokenize_text(indic_string):\n",
    "    tokens= indic_tokenize.trivial_tokenize(indic_string)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Text to index functions\n",
    "def hindi_to_indices(hindi_text):\n",
    "    indices = [hindi_vocab.get(token, hindi_vocab['<unk>']) for token in tokenize_text(hindi_text)]\n",
    "#     print(indices)\n",
    "    # indices2 = [hindi_vocab['<start>']] + indices + [hindi_vocab['<end>']]\n",
    "    indices2 =  indices + [hindi_vocab['<end>']]\n",
    "    return indices2\n",
    "\n",
    "def bhili_to_indices(bhili_text):\n",
    "    indices = [bhili_vocab.get(token, bhili_vocab['<unk>']) for token in tokenize_text(bhili_text)]\n",
    "    indices3 = [bhili_vocab['<start>']] + indices + [bhili_vocab['<end>']]\n",
    "    return indices3\n",
    "class Seq2SeqHinBhiliDataset(Dataset):\n",
    "    def __init__(self, data, hindi_vocab, bhili_vocab):\n",
    "        self.data = data.reset_index(drop=True)  # Reset indices to be continuous\n",
    "        self.hindi_vocab = hindi_vocab\n",
    "        self.bhili_vocab = bhili_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hindi_text = self.data.iloc[idx]['Hindi']\n",
    "        bhili_text = self.data.iloc[idx]['Bhili']\n",
    "        \n",
    "        hindi_indices = hindi_to_indices(hindi_text)\n",
    "        bhili_indices = bhili_to_indices(bhili_text)\n",
    "        \n",
    "        return {\n",
    "            'hindi_indices': torch.tensor(hindi_indices),\n",
    "            'bhili_indices': torch.tensor(bhili_indices)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    hindi_indices = [item['hindi_indices'] for item in batch]\n",
    "    bhili_indices = [item['bhili_indices'] for item in batch]\n",
    "\n",
    "    # Pad sequences to the length of the longest sequence in the batch\n",
    "    hindi_indices_padded = torch.nn.utils.rnn.pad_sequence(hindi_indices, batch_first=True, padding_value=2)\n",
    "    bhili_indices_padded = torch.nn.utils.rnn.pad_sequence(bhili_indices, batch_first=True, padding_value=2)\n",
    "\n",
    "    return {\n",
    "        'hindi_indices': hindi_indices_padded,\n",
    "        'bhili_indices': bhili_indices_padded\n",
    "    }\n",
    "\n",
    "print(len(df_train), len(df_val),end=\"\\n\\n\\n\")\n",
    "print(df_train.info(), df_val.info())\n",
    "df_train.describe()\n",
    "batch_size= 32\n",
    "dataset_train = Seq2SeqHinBhiliDataset(df_train, hindi_vocab, bhili_vocab)\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "dataset_dev = Seq2SeqHinBhiliDataset(df_val, hindi_vocab, bhili_vocab)\n",
    "data_loader_dev = DataLoader(dataset_dev, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "num_data_points_train = len(dataset_train)\n",
    "print(num_data_points_train)\n",
    "print(len(dataset_dev))\n",
    "sample_batch = next(iter(data_loader_train))\n",
    "print(sample_batch['hindi_indices'])\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        print(f'enc dim : {enc_hid_dim} | dec dim : {dec_hid_dim}')\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [batch_size, dec_hid_dim]\n",
    "        # encoder_outputs: [src_len, batch_size, enc_hid_dim * 2]\n",
    "        # print(f'inside attention hidden : {hidden.shape} | encoder_outputs : {encoder_outputs.shape}')\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        # print(f'inside_attention src_len {src_len}')\n",
    "        # Repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        # print(f'inside attention hidden {hidden.shape}')\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # print(f'inside_attention endoder ouput {encoder_outputs.shape}')\n",
    "\n",
    "        # print(f'concat {torch.cat((hidden, encoder_outputs), dim=2).shape}')\n",
    "        # Calculate energy\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "\n",
    "        # attention: [batch_size, src_len]\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "\n",
    "        return F.log_softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, n_layers=1):\n",
    "        # ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT\n",
    "        super().__init__()\n",
    "        # print(embedding_matrix.shape)\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, enc_hid_dim, num_layers=n_layers, bidirectional=True, dropout=dropout if n_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # print(\"******************encoder**********************\")\n",
    "        # src shape: [src_len, batch_size]\n",
    "        # print(f'inside encoder src shape {src.shape}')\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded shape: [src_len, batch_size, emb_dim]\n",
    "        # print(f'inside encoder embedded shape {embedded.shape}')\n",
    "        # embedded_ = embedded.permute(1,0,2)\n",
    "#         print(f'embedded_shape {embedded.shape}')\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs shape: [src_len, batch_size, enc_hid_dim * num_directions]\n",
    "        # hidden, cell shape: [num_layers * num_directions, batch_size, enc_hid_dim]\n",
    "        # print(f'encoder_hidden_shape {hidden.shape} | encoder_cell_shape {cell.shape} | outputs {outputs.shape}')\n",
    "\n",
    "        # Concatenate the hidden states from both directions\n",
    "        hidden = torch.cat((hidden[0], hidden[1]), dim=-1)\n",
    "        cell = torch.cat((cell[0], cell[1]), dim=-1)\n",
    "        # hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
    "        # cell = torch.tanh(self.fc(torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1)))\n",
    "        # print(f\"final_encoder_output_hidden {hidden.shape}\")\n",
    "        # print(f\"final_encoder_output_cell {cell.shape}\")\n",
    "        # print(\"***********************************************\")\n",
    "\n",
    "        # print(outputs)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "\n",
    "#         self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix_bhili, dtype=torch.float),freeze=False)\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM((enc_hid_dim * 2) + emb_dim, dec_hid_dim, dropout=dropout)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(0)  # input: [1, batch_size]\n",
    "        # print(f'input inside  decoder {input.shape}')\n",
    "        embedded = self.dropout(self.embedding(input))  # embedded: [1, batch_size, emb_dim]\n",
    "        # print(f'embedded inside decoder {embedded.shape}')\n",
    "        # Calculate attention weights\n",
    "        # print(hidden.shape, encoder_outputs.shape)\n",
    "        attention = self.attention(hidden, encoder_outputs)  # attention: [batch_size, src_len]\n",
    "\n",
    "        # Apply attention weights to encoder outputs\n",
    "        attention = attention.unsqueeze(1)  # attention: [batch_size, 1, src_len]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # encoder_outputs: [batch_size, src_len, enc_hid_dim * 2]\n",
    "        weighted = torch.bmm(attention, encoder_outputs)  # weighted: [batch_size, 1, enc_hid_dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)  # weighted: [1, batch_size, enc_hid_dim * 2]\n",
    "\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)  # rnn_input: [1, batch_size, (enc_hid_dim * 2) + emb_dim]\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden.unsqueeze(0), cell.unsqueeze(0)))\n",
    "\n",
    "        # Prepare input for fully connected layer\n",
    "        output = output.squeeze(0)  # output: [batch_size, dec_hid_dim]\n",
    "        embedded = embedded.squeeze(0)  # embedded: [batch_size, emb_dim]\n",
    "        weighted = weighted.squeeze(0)  # weighted: [batch_size, enc_hid_dim * 2]\n",
    "\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        # print(f\"embedded shape: {embedded.shape}\")\n",
    "        # print(f\"weighted shape: {weighted.shape}\")\n",
    "        return prediction, hidden.squeeze(0), cell.squeeze(0), attention.squeeze(1)\n",
    "\n",
    "\n",
    "\n",
    "        # src_ = src.permute(1,0)\n",
    "        # trg_ = trg.permute(1,0)\n",
    "        # # print(f'src {src.shape} | src_ {src_.shape}')\n",
    "        # # print(f'trg {trg.shape} | trg_ {trg_.shape}')\n",
    "        # batch_size = trg_.shape[1]\n",
    "        # trg_len = trg_.shape[0]\n",
    "\n",
    "        # # print(f'batch_size {batch_size} | trg_len {trg_len}')\n",
    "        # trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.3):\n",
    "        src_ = src.permute(1,0)\n",
    "        trg_ = trg.permute(1,0)\n",
    "        # print(trg_)\n",
    "        batch_size = trg_.shape[1]\n",
    "        trg_len = trg_.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(trg_len-1, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src_)\n",
    "\n",
    "        input = trg_[0,:]\n",
    "        # input = torch.tensor(2).unsqueeze(0).to(device)\n",
    "        # print(f'input outside loop {input}')\n",
    "\n",
    "        for t in range(0, trg_len-1):\n",
    "            # print(f'input seq to seq {input}')\n",
    "            output, hidden, cell, _ = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg_[t+1,:] if teacher_force else top1\n",
    "\n",
    "\n",
    "\n",
    "        return outputs\n",
    "# ENC_EMB_DIM = 64  # Embedding dimension for encoder\n",
    "# DEC_EMB_DIM = 64  # Embedding dimension for decoder\n",
    "# ENC_HID_DIM = 64  # Hidden dimension for encoder\n",
    "# DEC_HID_DIM = 128 # Hidden dimension for decoder \n",
    "# ENC_DROPOUT = 0.5  # Dropout rate for encoder|\n",
    "# DEC_DROPOUT = 0.5  # Dropout rate for decoder\n",
    "\n",
    "\n",
    "ENC_EMB_DIM = 16  # Embedding dimension for encoder\n",
    "DEC_EMB_DIM = 16  # Embedding dimension for decoder\n",
    "ENC_HID_DIM = 16  # Hidden dimension for encoder\n",
    "DEC_HID_DIM = 32 # Hidden dimension for decoder \n",
    "ENC_DROPOUT = 0.5  # Dropout rate for encoder\n",
    "DEC_DROPOUT = 0.5  # Dropout rate for decoder\n",
    "\n",
    "OUTPUT_DIM = len(bhili_vocab)  # Output dimension is the size of formula vocabulary\n",
    "INPUT_DIM= len(hindi_vocab)\n",
    "print(INPUT_DIM, OUTPUT_DIM)\n",
    "\n",
    "config = {\n",
    "    \"ENC_EMB_DIM\": ENC_EMB_DIM,\n",
    "    \"DEC_EMB_DIM\": DEC_EMB_DIM,\n",
    "    \"ENC_HID_DIM\": ENC_HID_DIM,\n",
    "    \"DEC_HID_DIM\": DEC_HID_DIM\n",
    "}\n",
    "\n",
    "# Writing to a JSON file\n",
    "with open('config.json', 'w') as json_file:\n",
    "    json.dump(config, json_file, indent=4)\n",
    "\n",
    "# Instantiating the model components\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "attention = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attention)\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "\n",
    "# Seq2Seq model\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "# Optimizer\n",
    "\n",
    "initial_learning_rate = 1e-3\n",
    "later_learning_rate = 1e-4\n",
    "switch_epoch = 75  # Epoch at which to switch learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_learning_rate, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "# Loss function, ignoring the index of the padding\n",
    "PAD_IDX = 2\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.85, patience=5)\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "loss_data = {\n",
    "    'train_losses': train_losses,\n",
    "    'valid_losses': valid_losses\n",
    "}\n",
    "\n",
    "N_EPOCHS = 350\n",
    "CLIP = 1\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()  \n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(tqdm(iterator)):\n",
    "        input_ids = batch['hindi_indices'].to(device)\n",
    "\n",
    "        trg = batch['bhili_indices'].to(device)\n",
    "        \n",
    "        # print(input_ids.shape, trg.shape)\n",
    "       \n",
    "        trg_input = trg\n",
    "        trg_output = trg[:, 1:]\n",
    "        # trg_new = trg[:, 1:]\n",
    "        # print(f'trg_out : {trg_output.shape}')\n",
    "        # print(f'trg_output : {trg_output}')\n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        # print(hindi_tensor_to_hindi_words(input_ids))\n",
    "        # print(bhili_tensor_to_bhili_words(torch.tensor(trg)))\n",
    "        \n",
    "        output = model(input_ids, trg)\n",
    "        # print(f'output {output.shape}')\n",
    "\n",
    "        # Reshape output and target tensors to compute loss\n",
    "        output_dim = output.shape[-1]\n",
    "        # output = output.contiguous().view(-1, output_dim)\n",
    "        # trg_output = trg_output.contiguous().view(-1)\n",
    "        output=output.permute(1,2,0)\n",
    "        # Calculate loss\n",
    "        # print(output, trg_output)\n",
    "        # print(f\"I/p id is: {input_ids}, Model ouput is: {output} Target is {trg_output}\")\n",
    "        loss = criterion(output, trg_output)\n",
    "#         print(loss)\n",
    "        loss.backward() \n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)  # Gradient clipping\n",
    "\n",
    "        optimizer.step() \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch['hindi_indices'].to(device)\n",
    "            trg = batch['bhili_indices'].to(device)\n",
    "            # print(src)\n",
    "\n",
    "            trg_new = trg[:, 1:]  \n",
    "\n",
    "            output = model(src, trg, 0)  # Turn off teacher forcing\n",
    "            # print(f'output shape {output.shape}')\n",
    "            # print(output)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.permute(1, 2, 0)  \n",
    "\n",
    "            loss = criterion(output, trg_new)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # Adjust learning rate after the switch_epoch\n",
    "    if epoch == switch_epoch:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = later_learning_rate\n",
    "    \n",
    "    train_loss = train(model, data_loader_train, optimizer, criterion)\n",
    "    valid_loss = evaluate(model, data_loader_dev, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} | Train Loss: {train_loss:.3f} | Val Loss : {valid_loss:.3f}')\n",
    "\n",
    "    # Save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'valid_losses': valid_losses,\n",
    "            'best_valid_loss': best_valid_loss\n",
    "        }\n",
    "        torch.save(checkpoint, f'/kaggle/working/chunked_speeches_attention_0.6_checkpoint_final_800ep.pt')\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), '/kaggle/working/chunked_speeches_attention_model_0.6_v700ep.pt')\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot the train and validation loss\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "\n",
    "# Mark the first train and validation loss values\n",
    "plt.scatter(0, train_losses[0], color='red')  # First train loss value\n",
    "plt.text(0, train_losses[0], f'{train_losses[0]:.2f}', color='red', fontsize=10, ha='left', va='bottom')\n",
    "\n",
    "plt.scatter(0, valid_losses[0], color='blue')  # First validation loss value\n",
    "plt.text(0, valid_losses[0], f'{valid_losses[0]:.2f}', color='blue', fontsize=10, ha='left', va='top')\n",
    "\n",
    "# Mark the last train and validation loss values\n",
    "last_epoch = len(train_losses) - 1\n",
    "plt.scatter(last_epoch, train_losses[-1], color='red')  # Last train loss value\n",
    "plt.text(last_epoch, train_losses[-1], f'{train_losses[-1]:.2f}', color='red', fontsize=10, ha='right', va='bottom')\n",
    "\n",
    "plt.scatter(last_epoch, valid_losses[-1], color='blue')  # Last validation loss value\n",
    "plt.text(last_epoch, valid_losses[-1], f'{valid_losses[-1]:.2f}', color='blue', fontsize=10, ha='right', va='top')\n",
    "\n",
    "# Add labels, title, and grid\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# Save the plot as a JPG file\n",
    "plt.savefig('train_valid_loss_plot.jpg', format='jpg')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "print(len(hindi_vocab), len(bhili_vocab))\n",
    "class BeamSearchNode(object):\n",
    "    def __init__(self, hidden, previousNode, wordId, logProb, length):\n",
    "        self.hidden = hidden\n",
    "        self.prevNode = previousNode\n",
    "        self.wordid = wordId\n",
    "        self.logp = logProb\n",
    "        self.length = length\n",
    "\n",
    "    def eval(self, alpha=1.0):\n",
    "        reward = 0\n",
    "        # Adjust the length penalty\n",
    "        return self.logp / float(self.length - 1 + 1e-6) + alpha * reward\n",
    "    \n",
    "    \n",
    "def attn_text_to_indices(text, vocab):\n",
    "    tokens = ['<start>'] + tokenize_text(text + ['<end>'])  # Tokenize the text\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        indices.append(vocab.get(token, vocab[\"<unk>\"]))  # Use the token itself\n",
    "    return indices\n",
    "\n",
    "def attn_beam_search(model, src,device, beam_width=7, max_len=30):\n",
    "    src = src.to(device)\n",
    "    # print(src)\n",
    "    src = src.unsqueeze(1)  # [src_len, 1] for a single example\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden, cell = model.encoder(src)\n",
    "\n",
    "    # print(encoder_outputs.shape, hidden.shape, cell.shape)\n",
    "    # Starting Node - hidden state, previous node, word id, log prob, length\n",
    "    start_node = BeamSearchNode((hidden, cell), None, [torch.tensor(0)], 0, 1)\n",
    "    nodes = [start_node]\n",
    "\n",
    "    # Start beam search\n",
    "    for _ in range(max_len):\n",
    "        new_nodes = []\n",
    "        for node in nodes:\n",
    "            input = torch.LongTensor([node.wordid[-1]]).to(device)\n",
    "            hidden, cell = node.hidden  # Unpack hidden state\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Make sure to pass encoder_outputs for attention mechanism\n",
    "                output, hidden, cell,_ = model.decoder(input, hidden, cell, encoder_outputs)\n",
    "                log_probs = F.log_softmax(output, dim=1)\n",
    "\n",
    "            top_log_probs, top_idx = log_probs.topk(beam_width)\n",
    "            for i in range(beam_width):\n",
    "                word_idx = top_idx[0][i].item()\n",
    "                log_prob = top_log_probs[0][i].item()\n",
    "\n",
    "                new_node = BeamSearchNode((hidden, cell), node, node.wordid + [word_idx], node.logp + log_prob, node.length + 1)\n",
    "                new_nodes.append(new_node)\n",
    "\n",
    "        # Sort nodes by log probability\n",
    "        nodes = sorted(new_nodes, key=lambda node: node.logp, reverse=True)[:beam_width]\n",
    "\n",
    "    # Find the path with the highest probability\n",
    "    end_node = nodes[0]\n",
    "    output_sequence = end_node.wordid\n",
    "\n",
    "    # Convert indices to tokens\n",
    "    output_tokens = []\n",
    "    for i in output_sequence:\n",
    "        if i == bhili_vocab[\"<end>\"]:  # Assuming <eos> is your end of sequence token\n",
    "            break\n",
    "        elif i == bhili_vocab[\"<start>\"]:\n",
    "            continue\n",
    "        else:\n",
    "            output_tokens.append(list(bhili_vocab.keys())[list(bhili_vocab.values()).index(i)])\n",
    "    return output_tokens\n",
    "\n",
    "def make_sentence_from_list(list):\n",
    "    sentence= \"\"\n",
    "    for word in list:\n",
    "        sentence+= word+\" \"\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a0be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T22:29:02.705274Z",
     "iopub.status.busy": "2024-08-20T22:29:02.704227Z",
     "iopub.status.idle": "2024-08-20T22:29:02.711049Z",
     "shell.execute_reply": "2024-08-20T22:29:02.710128Z"
    },
    "papermill": {
     "duration": 10.004457,
     "end_time": "2024-08-20T22:29:02.713078",
     "exception": false,
     "start_time": "2024-08-20T22:28:52.708621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_bleu(reference_text, candidate_text):\n",
    "    reference_tokens = [list(reference_text)]  \n",
    "    candidate_tokens = list(candidate_text)\n",
    "    \n",
    "\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu_score = sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smoothie)\n",
    "    \n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "\n",
    "def calculate_chrf2(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate the CHRF2 score between two sentences using sacrebleu.\n",
    "    \"\"\"\n",
    "    score = sacrebleu.sentence_chrf(predicted, [actual], beta=1)\n",
    "    return score.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b9cd87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T22:29:22.644966Z",
     "iopub.status.busy": "2024-08-20T22:29:22.644603Z",
     "iopub.status.idle": "2024-08-20T22:31:39.930358Z",
     "shell.execute_reply": "2024-08-20T22:31:39.929432Z"
    },
    "papermill": {
     "duration": 149.953999,
     "end_time": "2024-08-20T22:31:42.793479",
     "exception": false,
     "start_time": "2024-08-20T22:29:12.839480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "582it [02:17,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results_validation.csv\n",
      "Average BLEU Score on validation dataset is: 16.858674968107216\n",
      "Average CHRF2 Score on validation dataset is: 10.60791840273717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ctr = 0\n",
    "bleu_score=0\n",
    "chrf2_score=0\n",
    "total_bleu_score=0\n",
    "total_chrf2_score=0\n",
    "results=[]\n",
    "\n",
    "for index, row in tqdm(df_val.iterrows()):\n",
    "\n",
    "    hindi_sentence = row['Hindi']\n",
    "    bhili_sentence = row['Bhili']\n",
    "    hindi_sentence_tensor = torch.tensor(hindi_to_indices(hindi_sentence))\n",
    "    # print(hindi_sentence_tensor)\n",
    "    bhili_sentence_list = attn_beam_search(model, hindi_sentence_tensor, \"cuda\")\n",
    "    bhili_sentence_predicted = make_sentence_from_list(bhili_sentence_list)\n",
    "    bleu_score= calculate_bleu(bhili_sentence, bhili_sentence_predicted)\n",
    "    chrf2_score= calculate_chrf2(bhili_sentence, bhili_sentence_predicted)\n",
    "    total_bleu_score+=bleu_score\n",
    "    total_chrf2_score+= chrf2_score\n",
    "\n",
    "    results.append({\n",
    "        \"Hindi\": hindi_sentence,\n",
    "        \"Actual Bhili\": bhili_sentence,\n",
    "        \"Predicted Bhili\": bhili_sentence_predicted,\n",
    "        \"BLEU Score\": bleu_score,\n",
    "        \"CHRF2 Score\": chrf2_score\n",
    "    })\n",
    "        \n",
    "    ctr += 1\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('/kaggle/working/results_validation.csv', index=False)\n",
    "\n",
    "print(\"Results saved to results_validation.csv\")\n",
    "average_bleu_score= 100* total_bleu_score/ctr\n",
    "average_chrf2_score= total_chrf2_score/ctr\n",
    "print(f\"Average BLEU Score on validation dataset is: {average_bleu_score}\")\n",
    "print(f\"Average CHRF2 Score on validation dataset is: {average_chrf2_score}\")\n",
    "\n",
    "\n",
    "\n",
    "# ctr = 0\n",
    "# bleu_score=0\n",
    "# chrf2_score=0\n",
    "# total_bleu_score=0\n",
    "# total_chrf2_score=0\n",
    "# results=[]\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "\n",
    "#     hindi_sentence = row['Hindi']\n",
    "#     bhili_sentence = row['Bhili']\n",
    "#     hindi_sentence_tensor = torch.tensor(hindi_to_indices(hindi_sentence))\n",
    "#     # print(hindi_sentence_tensor)\n",
    "#     bhili_sentence_list = attn_beam_search(model, hindi_sentence_tensor, \"cuda\")\n",
    "#     bhili_sentence_predicted = make_sentence_from_list(bhili_sentence_list)\n",
    "#     bleu_score= calculate_bleu(bhili_sentence, bhili_sentence_predicted)\n",
    "#     chrf2_score= calculate_chrf2(bhili_sentence, bhili_sentence_predicted)\n",
    "#     total_bleu_score+=bleu_score\n",
    "#     total_chrf2_score+= chrf2_score\n",
    "\n",
    "\n",
    "#     results.append({\n",
    "#         \"Hindi\": hindi_sentence,\n",
    "#         \"Actual Bhili\": bhili_sentence,\n",
    "#         \"Predicted Bhili\": bhili_sentence_predicted,\n",
    "#         \"BLEU Score\": bleu_score,\n",
    "#         \"CHRF2 Score\": chrf2_score\n",
    "#     })\n",
    "        \n",
    "#     ctr += 1\n",
    "\n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv('/kaggle/working/results_combined.csv', index=False)\n",
    "\n",
    "# print(\"Results saved to results_combined.csv\")\n",
    "# average_bleu_score= total_bleu_score/ctr\n",
    "# average_chrf2_score= total_chrf2_score/ctr\n",
    "# print(f\"Average BLEU Score on combined dataset is: {average_bleu_score}\")\n",
    "# print(f\"Average CHRF2 Score on combined dataset is: {average_chrf2_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a36a30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T22:32:02.731088Z",
     "iopub.status.busy": "2024-08-20T22:32:02.730451Z",
     "iopub.status.idle": "2024-08-20T22:32:02.734856Z",
     "shell.execute_reply": "2024-08-20T22:32:02.733917Z"
    },
    "papermill": {
     "duration": 9.856357,
     "end_time": "2024-08-20T22:32:02.737677",
     "exception": false,
     "start_time": "2024-08-20T22:31:52.881320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f84d7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T22:32:22.836845Z",
     "iopub.status.busy": "2024-08-20T22:32:22.836079Z",
     "iopub.status.idle": "2024-08-20T22:34:19.308176Z",
     "shell.execute_reply": "2024-08-20T22:34:19.307097Z"
    },
    "papermill": {
     "duration": 126.560779,
     "end_time": "2024-08-20T22:34:19.310176",
     "exception": false,
     "start_time": "2024-08-20T22:32:12.749397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473\n",
      "473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "473it [01:56,  4.06it/s]\n"
     ]
    }
   ],
   "source": [
    "def infer(hindi_sentence):\n",
    "    hindi_sentence_tensor = torch.tensor(hindi_to_indices(hindi_sentence))\n",
    "    bhili_sentence_list = attn_beam_search(model, hindi_sentence_tensor, \"cuda\")\n",
    "    bhili_sentence_predicted = make_sentence_from_list(bhili_sentence_list)  \n",
    "    \n",
    "    results.append({\n",
    "        \"Hindi\": hindi_sentence,\n",
    "        \"Predicted Bhili\": bhili_sentence_predicted,\n",
    "    })\n",
    "    \n",
    "    # Append the results to a CSV file\n",
    "    with open('speech_predictions.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"Hindi\", \"Predicted Bhili\"])\n",
    "        \n",
    "        # Write the header only once\n",
    "        if file.tell() == 0:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        writer.writerow({\n",
    "            \"Hindi\": hindi_sentence,\n",
    "            \"Predicted Bhili\": bhili_sentence_predicted,\n",
    "        })\n",
    "    \n",
    "    return bhili_sentence_predicted\n",
    "  \n",
    " \n",
    "    \n",
    "\n",
    "\n",
    "df= pd.read_csv('/kaggle/input/comb-modi/Speech_2024 - Sheet1.csv')\n",
    "# df = df.drop('Unnamed: 2', axis=1)\n",
    "print(len(df))\n",
    "df.dropna()\n",
    "print(len(df))\n",
    "\n",
    "# Identify duplicates in each column\n",
    "duplicates_col1 = df.duplicated('Hindi', keep=False)\n",
    "# duplicates_col2 = df.duplicated('Bhili', keep=False)\n",
    "\n",
    "# Combine the conditions to identify rows where either column has duplicates\n",
    "duplicates_any_col = duplicates_col1\n",
    "df_cleaned = df[~duplicates_any_col]\n",
    "\n",
    "# len(df_cleaned)\n",
    "# df= df_cleaned\n",
    "\n",
    "df_val= df_cleaned\n",
    "\n",
    "ctr = 0\n",
    "bleu_score=0\n",
    "chrf2_score=0\n",
    "total_bleu_score=0\n",
    "total_chrf2_score=0\n",
    "results=[]\n",
    "\n",
    "for index, row in tqdm(df_val.iterrows()):\n",
    "\n",
    "    hindi_sentence = row['Hindi']\n",
    "    # bhili_sentence = row['Bhili']\n",
    "    predicted_sentence= infer(hindi_sentence)\n",
    "    ctr+=1\n",
    "#     print(f\"{ctr} sentences processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680889bd",
   "metadata": {
    "papermill": {
     "duration": 10.141877,
     "end_time": "2024-08-20T22:34:39.419565",
     "exception": false,
     "start_time": "2024-08-20T22:34:29.277688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df74a5",
   "metadata": {
    "papermill": {
     "duration": 10.264122,
     "end_time": "2024-08-20T22:34:59.545331",
     "exception": false,
     "start_time": "2024-08-20T22:34:49.281209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237d687",
   "metadata": {
    "papermill": {
     "duration": 10.063552,
     "end_time": "2024-08-20T22:35:19.504921",
     "exception": false,
     "start_time": "2024-08-20T22:35:09.441369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5539895,
     "sourceId": 9168074,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5569925,
     "sourceId": 9211449,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28720.764792,
   "end_time": "2024-08-20T22:35:31.156759",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-20T14:36:50.391967",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
